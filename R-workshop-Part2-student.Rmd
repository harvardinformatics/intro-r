---
title: "[Workshop] Introduction to R Part 2: tidyverse"
description: "An introduction to 'tidy' data and how to manipulate data with the tidyverse family of R packages."
date: "September 24, 2025"
authors: 
  - Adam Freedman
  - Lei Ma
  - Gregg Thomas
output: 
  html_document:
    keep_md: true
---

# Introduction to R Part 2: Data manipulation with tidyverse

Welcome to Day 2 of our Introduction to R workshop! If you're viewing this file on the website, you are viewing the final, formatted version of the workshop. The workshop itself will take place in the RStudio program and you will *edit and execute the code in this file*. Please download the raw file [here :octicons-download-24:](R-workshop-Part2-student.Rmd)

## What is tidy data?

The notion of tidy data has been around in various incarnations for a while but has more recently been formalized in this [paper :octicons-download-24:](https://vita.had.co.nz/papers/tidy-data.pdf){:target="\_blank"} by Hadley Wickham, the developer of *tidyverse*. Tidy data are, in short, data that have been organized in a consistent way that makes data exploration and visualization easier. Two key principles are that:

-   Each variable forms a column
-   Each observation forms a row

Organized this way, for analysis with software such as R, values for different variables can be linked within observations, e.g. rows of our example data from day 1 are observations of individual penguins, and columns contain information about each individual (species, sex, island where recorded, body mass, etc.) such that one could explore how the weight or flipper length varies by sex and species. Often times data that one pulls down from a repository (or that is provided by a collaborator) are not tidy! For example, multiple variables may be stored in the same column, such as when two different treatment types are concatenated into a single string. Tidying such data would require separating each treatment into a separate column.

## What is *tidyverse*?

*Tidyverse* is a collection of **R packages** designed to be used together to clean up data sets and make data exploration and visualization easier, and to make data "tidy". Because all of the R commands and exercises below will involve functions available in tidyverse, you need to have tidyverse installed.

### What are packages?

R packages are extensions to the base R environment. They typically contain code consisting of functions available in the package, documentation, and in some cases data sets. R packages are built following a standard format so R will interact with them in a reliable, consistent way.

### Downloading and installing R packages

Some packages are included with the base R installation. You can see the packages that come with R by default (or that you may have installed) if you click over to the *Packages* tab in your Rstudio window. However, many packages you will want to use for data analysis need to be downloaded. R packages generally come from one of two places:

-   [CRAN :octicons-link-external-24:](https://cran.r-project.org/){:target="\_blank"}: default source for "official" R packages
-   [Bioconductor :octicons-link-external-24:](https://www.bioconductor.org/){:target="\_blank"}: specialized packages for analyses of biological data

How you install packages will depend on where you are getting the package from. For *CRAN*, you can use the R function `install.packages()` or the Rstudio Tools -\> Install Packages menu. For example, if you wanted to install the *abc* package for doing Approximate Bayesian Computation, in the R console you would simply type: *install.packages("abc")*.

To install *Bioconductor* packages, you will need to first install the Bioconductor package manager (BUT DON"T DO THIS NOW!): `if (!require("BiocManager", quietly = TRUE)) install.packages("BiocManager") BiocManager::install(version = "3.16")`

Once the package manager is installed, you can install a Bioconductor project packages with the following syntax: `BiocManager::install(<name of Bioconductor R package>)`. We won't discuss Bioconductor more in this workshop, but it is a useful resource to be aware of.

Packages frequently have other R packages as dependencies, and so you will often get the prompt from R asking if you want to update (the dependencies) to their current version, and if you want to compile packages that require compilation from source. Packages compiled from source can be particularly prone to installation failures, especially on M1/M2 Macs, and often require some searching to decode how to proceed. But today we will only use packages that are simple and easy to install.

So we mentioned that *tidyverse* is a collection of R packages. You can conveniently install all these related packages with one install command.

> If you haven't already installed *tidyverse* and the *palmerpenguins" dataset, do that now by executing the following code block.

```{r,eval=FALSE}

install.packages("tidyverse")

```

Note that installing a package is not the same thing as loading it into your current R session. Once the package is installed, you have the functions on your computer, but in order to use them in your R session, you need to load the package, which we usually do with the `library` function.

> Then load *tidyverse* by running the following code block

```{r}

library(tidyverse)
```

For more information, go to [tidyverse :octicons-link-external-24:](https://www.tidyverse.org/){:target="\_blank"}.

Packages usually come with *vignettes*, which give you short introductions and tutorials for their use. You can find these on the web, but you can also browse them in R.

> Run the following code block to see the vignettes for the **dplyr** package, which is part of the tidyverse

```{r, eval=FALSE}

browseVignettes(package = "dplyr")

```

## Our data
Today's data set comes from... wait for it ... penguins! In particular, we are using the [Palmer penguins :octicons-link-external-24:](https://allisonhorst.github.io/palmerpenguins/){:target="\_blank"} data set contains measurement data for 3 species of penguins collected on three islands at the LTER site in the Palmer Archipelago, Antarctica.

This data is available as an R package which we can install as follows:

```{r,eval=FALSE}
install.packages("palmerpenguins")
```

## An introduction to data frames and "tibbles"
In most cases, tabular data are represented as a "data.frame", where the expectation is that, in line with the notion of "tidy" data, rows are observations and columns are variables for which there are values recorded for each observation, including the absence of an observation, i.e. missing data. Historically in R and prior to tidyverse,one would load a data frame from a file with a file reading function such as `read.csv()` or `read.table()`, and one would have to explicitly specify whether or not the first row represented the variable names for each column, whether there were row names, etc. Tidyverse has its own data load functions, `read_csv()` and `read_table()` which load tabular data as "tibbles". *tibbles* are effectively just *data.frames* with some improvements to how they are displayed, how variables can be accessed and a few other minor, subtle differences. But you can think of them as pretty data.frames.

Because the data set we are using today already comes built into the *palmerpenguins* package, the *penguins* dataframe is immediately available after loading the `palmerpenguins` R package. In most use cases, your data are stored as a text file in some sort of tabular format--space, tab or comma separated--and tidyverse has file loading functions. To show how loading works, we will write the `penguins` dataframe from *palmerpenguins* to a csv file, the use the tidyverse `read_csv` function to load it.

> Run the following code block to write the peguins data frame to a file then load it as a tibble

```{r}
library(palmerpenguins)
write.csv(penguins,file="penguins.csv")
mypenguins<-read_csv("penguins.csv")
```

Tidyverse file loading functions assume the first row of your data contains column names, while the base R functions (e.g. `read.csv` and `read.table`) do not. Tidyverse file loaders return a tibble rather than a standard dataframe.  One of the first things you will notice is that upon loading you will get a brief summary of the tibble contents, such as the tibble dimensions, the column separator, and the names of variables (columns) belonging to each type: *dbl* for numeric and *chr* for character. We can also take a quick peek at the first few rows by simply calling the tibble. 

> Run the following code block to see some basic information about this data.

```{r}
mypenguins
```

In the R markdown, you will notice that in this view there are tabs at the bottom right for advancing to additional sets of rows. If there were too many columns to be displayed in the window, there would also be an arrow at the top right allowing you to advance to the "right" to see additional columns.sa

One theme of tidyverse is that it provides replacements for a lot of base R functions with better formatting. A tibble is one example of this; the `glimpse()` function is another; it is basically a pretty version of `str()`.

> Run the following code block to get a "glimpse" of the data:

```{r}

glimpse(mypenguins)

```

We can of course use all our non-tidyverse (base R) functions to look at data, like `head` and `View`, since tibbles are just pretty data frames.

> To get the first 10 lines of mypenguins, run the following code block:

```{r}

head(mypenguins, 10)

```

For the rest of the workshop, we will just use the `penguins` tibble provided with *palmerpenguins* rather than our newly created `mypenguins` tibble, as they are identical. It is also worth noting that, if you are using other tools in R that produce data frames, you can use the `as_tibble` function to convert the dataframe to a tibble on the fly. Similarly,if your collaborator has provided you with messy data that is not immediately amenable to loading as a tibble, you can use a standard data load, do some relevant manipulations, then use the `as_tibble` function. 

## A simple tidyverse function: sorting a tibble on a column value

Today we'll talk about a bunch of functions that are part of tidyverse and provide convenient ways to manipulate tibbles. While we'll discuss this in the context of tibbles, everything here works on data frames too - remember a tibble is just a pretty data frame.

Tidyverse functions can get very complex, but they share a common grammar. We'll see the real power of this on day 3 when we talk about the grammar of graphics using *ggplot*, but getting a handle on the basic grammar of the tidyverse can help you a lot with data manipulation as well.

We'll start with something really simple: sorting a data frame. We do this with the `arrange()` function.

> To sort the tibble by species, run the following code block:

```{r}

arrange(penguins, species)

```

All tidyverse functions take the tibble or data frame to operate on as their first argument. The second argument ('species') is what makes tidyverse special. Note that we are using the syntax we've used before to refer to objects, here: a simple unquoted word or variable name. 

What happens if we just type `species` in the **Console**?

Tidyverse allows you to treat the **variables in your current data frame** as if they were **objects**. That is, within the arrange function (or almost any tidyverse function), you can reference the names of columns in your data frame and magically the function understands what you mean.

What if we want to sort by multiple values? For example, first sort by section, and then sort by year?

Many tidyverse functions have a special second argument that you will see listed in the help pages as `...`. You can look at `?arrange` as an example. This means that the second argument to these functions can be any number of columns, and the function will work on those columns in order. So to sort by year and then sex (i.e. sex within year):

> To sort the tibble by year, then by sex, run the following code block:

```{r}

arrange(penguins, year, sex)

```

In this case, sorting is done first by *year*, and then *sex*. Alternately, you could sort year in descending order and sex in ascending (alphabetic) order:

> To sort the tibble in descending order by year, and ascending order by sex, run the following code block:

```{r}

arrange(penguins, desc(year),sex)

```

`desc()` here is a helper function that turns year into a descending sort instead of ascending. Tidyverse has a lot of these, although not many are applicable to `arrange()` we'll introduce more as we work through today.

Note that so far we haven't stored the sorted data. Let's try that now.

> \*\*Sorting exercise\*. Sort your `penguins` tibble by species, then by year, then by body mass, and store the output in a new tibble called `sorted_penguins`.

```{r}

```

## Selecting a subset of columns

Sometimes in a big data set, there are a lot of columns that are not germane to the analysis at hand, such that one can simply select columns that represent the variables you want to work with. To do this we use the `select()` function.

For example, let's say you are only interested in looking at the relationship between flipper length and body mass, irrespective of year and island, such that you only want to retain those two morphological measurements, sex, and species

> To select these variables from the *penguins* tibble, run the following code block:

```{r}

select(penguins, species, sex, flipper_length_mm, body_mass_g)

```

The first argument to `select()` is the tibble one is subsetting from, and the rest of the arguments are the columns we wanted to keep. We see that `select` produced a tibble with the same number of rows as the *penguins* tibble, but only with the columns we chose to include. Note that, the above implementation of `select()` **DOES NOT** produce a new tibble: it simply prints the result to the screen. If we want to store the output, we need to redirect to a new tibble.

> To produce a new tibbble consisting of *species*, *sex*, *flipper_length_mm*, *body_mass_g*, run the following code block:

```{r}

flipper_mass_data <- select(penguins, species, sex, flipper_length_mm, body_mass_g)
flipper_mass_data

```

The `select` function, you should notice, works kind of like indexes we talked about yesterday. But it is a lot easier to use, because you can refer to the column names as variables in your command. Tidyverse magic at work!

Alternatively, and this gets more useful when we have a lot of columns we want to keep and only a few we want to discard, we can dump columns rather than select *for* them. We can do this by applying a logical "NOT" by placing a *!* in front of a vector of variables we want to filter out.

> To exclude the *island*, *bill_length_mm*, and *bill_depth_mm* variables, run the following code block:

```{r}

select(penguins, !c(island, bill_length_mm, bill_depth_mm))

```

> To convince yourself those variables actually got filtered out, run `glimpse()` in the code block below:

```{r}

glimpse(select(penguins, !c(island, bill_length_mm, bill_depth_mm)))

```

Again, in this example we have NOT yet created a new tibble for downstream data analysis, we have simply printed the result of `select()` to the screen. We would have to redirect the output of this function to a new tibble to save the result.

> To generate the new tibble, run the following code block:

```{r}

noyear_bill_island <- select(penguins, !c(island, bill_length_mm, bill_depth_mm))

```


> **Column selection exercise:**
>
> 1.  As a prelude to quantifying the relationship within species between bill traits, construct a new tibble called *bill_data* that includes the following variables: *species*, *sex*, *bill_length_mm*, and *bill_depth_mm*:

```{r}


```

With large data frames with many columns, it can often be difficult to type out a list of everything you want. Conveniently, select comes with its own helper functions to allow you to pick columns. For example, the `starts_with()` function generates a list of all the columns that start with a particular word in the current data frame.

> To see how `starts_with`, works, run the following code block:

```{r}

select(penguins, species, sex, starts_with("bill"))

```

There are lots more of these: `?select` is your friend here.

## Selecting a subset of rows based upon values of variables

Up until now, we have been selecting columns to remove or keep. Often times, one wants to restrict analysis and visualization to a subset of the observations, that is, to select particular **rows**. In the case of our data set, perhaps you are only interested in cutthroat trout, and only those that occur in old growth forest. The `filter()` function allows you to select **rows** based upon values in multiple columns, similar to how we talked about logical vectors yesterday. This function works by only returning values where the condition is true. It not only excludes those for which it is FALSE, it also filters out rows where there is missing data (i.e. NA) for the column evaluated (because any logical test evaluates to false when given NA).

Let us try creating a new tibble, with data restricted to Adelie penguins.

> Create a new tibble, with data restricted to trout by running the code block below:

```{r}

adelie <- filter(penguins, species=="Adelie")
adelie

```

Note the `==`, as we discussed yesterday for logical operations. We can add additional logical conditions for other variables if we want to make more complex selections. For example, we might only want to look at male Adelie penguins. Let's do this:

> To select rows for trout in old growth forest, run the following code block:

```{r}

adelie_males <- filter(penguins, species=="Adelie", sex =="male")
adelie_males

```

When we provide more than one filtering criterion, the commas between those criteria are equivalent of *AND* in a logical statement. The filter command above effectively says: return rows where species == "Adelie" AND sex == "male".

Similarly, we can use "\|" to specify either  in a logical filtering step. For example if we wanted records for either Adelie OR Gentoo penguins,  we can specify two acceptable values for *species*.

> To make this type of *OR* selection, run the code block below:

```{r}

adelie_gentoo <- filter(penguins, species=="Adelie" | species == "Gentoo")
adelie_gentoo

```

> **Row selection exercise:** Create a new tibble that only contains Chinstrap penguins, that are females, for years after 2007)

```{r}


```

Remember that in order to select a set of rows, one is performing a logical operation, effectively seeing if the values of a column or a set of columns in a row meets the specified criterion, constructing a logical vector of TRUEs and FALSEs depending upon whether the criterion is met, and then printing (or redirecting to a new tibble) all columns but only the rows that are TRUE, i.e. where the criterion is met.

Let's try another, more complex example where we only want Adelie penguins, but only males from Biscoe or females from Torgersen. There is not a clear research question behind this sort of selection...but ... it is a useful example for how to string together logical/conditional statements to subset data.

> To do this, run the following code block:

```{r}

filter(penguins, species %in% c("Adelie"), (island == "Biscoe" &  sex == "male") | (island == "Torgersen" & sex=="female"))

```

While we could simply have filtered on `species==Adelie`, we demonstrate the `%in%` as a way to construct a logical statement meaning "IN the following vector", such that if there were multiple values for species, rather than stringing together a series of *OR* statements, we evaluate membership in the vector. In this case, in the construction of a logical criterion for rows, *&* is used to specify *AND* and *\|* is used to specify *OR*. It is *VERY IMPORTANT* to understand how R interprets these logical operators. In general, *AND* is *stronger* than *OR*, meaning the ANDs get interpreted first.

To clarify what we mean by this, imagine the following vector:

```{r}

test <- c(1,2,3)

```

Now look at the result of the following two logical operations:

```{r}

3 %in% test | 5 %in% test & 4 %in% test 
3 %in% test & 5 %in% test | 4 %in% test

```

In the first statement, the part after the "\|" , i.e the *AND* gets done first, and it is FALSE. The part before, evaluting whether 3 is in *test* is TRUE. So, the logical statement is saying TRUE \| FALSE, and in logical statments, if one of the options in an OR statment is TRUE, the statement returns TRUE. In the second statment, the condition before the "\|" has an AND, so it is evaluated first. Both numbers aren't in *test* so it is FALSE. The part to the right of the "\|" is also FALSE, thus FALSE OR FALSE returns FALSE.

For purposes of making our R code clearer -- and it is a good idea to do this in your R code -- when writing complex tibble filtering operations, we put the criteria joined by *AND* inside parentheses. Thus the statement is saying: "select rows for which species is in the vector of names that only include Adelie, and for which island equals BISCO *AND* sex equals male... *OR* ... island equals Torgersen and sex equals female.

of course, with a simple dataset such as the penguins data, we could use *OR* instead of the `%in%`.

> To use the *OR* statement to select multiple species with two different year-sex combinations, run the code block below:

```{r}

filter(penguins, (species == "Adelie" | species == "Gentoo"), (sex == "male" &  year == "2008") | (sex == "female" & year==2009))

```

> **Complex row selection exercise** Can you select only observations for males from before 2009, including Adelie penguins with a body mass greater than 4043 and Gentoo penguins with a body mass greater than 5485. These values are the mean mass for males of each species.

```{r}


```

## Creating new variables with mutate()

Often times, we want to perform analyses on synthetic variables that are derived from the original variables in a data set. For example, one might want to look at one variable normalized by the mean value of that variable, or one variable divided by another.

In this data set, we have bill length (bill_length_mm) and bill depth (bill_depth_mm). Let's add a column variable to our tibble that is depth divided by length, which might be viewed as a bill size-adjusted depth. We'll do this with the `mutate()` function.

> To add a *depth_sizeadj* variable to *penguins*, run the code block below:

```{r}

penguins <- mutate(penguins, depth_sizeadj = bill_depth_mm/bill_length_mm)

```

In redirecting back to the *penguins* tibble, we are changing the input data rather than writing a new tibble with this variable added. Note also that this command will **overwrite** the body condition column, if it exists. This is often what is desired, but be aware of this behavior.

> **New variables creation exercise 1:** For male adelie penguins, we want to flag outlier observations where body mass is unusually large or small relative to the mean value. To do this, you will need to use the `mean()` and `sd()` functions to calculate mean and standard deviation of body mass. Once you've done that, create a new column that is the variable *mass_outlier* which is a boolean variable that indicates whether body mass is greater than or less than 2 standard deviations away from the mean, and add this to the *adelie_males* tibble. Finally, creat e a new tibble called *outliers* that only contains male Adelie penguins with outlier-classified body mass values.

This is a complicated exercise, but there are hints in the comments below. Note there are many ways to do this exercise, and the hints are just one way. You might find ways to combined steps, for example.

## Extract the mean body mass of male Adelie penguins from the `adelie_males` tibble
Create a new variable called mass_mean that stores the mean of the body_mass_g column; remember that you need to include the argument na.rm = TRUE to tell the mean function to remove missing values first

```{r}

```

## Create a new variable called tail_sd that stores the standard deviation of the mass column. The function for this is sd(); like mean() you need to include the argument na.rm = TRUE to tell the sd function to remove missing values first

```{r}

```

## Create a new column in the adelie_males tibble that is TRUE if body mass is more than two standard deviations from the mean

```{r}

```

## Filter the adelie_males tibble to keep only rows where mass_outlier is TRUE

```{r}

```
## You should have two observations in your outliers tibble. If you don't and you aren't sure what went wrong, put up your red sticky.

### Using `mutate()` to reformat missing values

Another important use of `mutate()` is to convert missing values. Data sets are frequently not constructed with R in mind, such that missing values might be formatted differently, e.g. -999 or "N/A". Of course, one wouldn't want to generate plots that included the value -999. While our example data already have missing data formatted in an R-compliant way, here is an example of how one would convert -999 to NA. We use `mutate()` combined with the `na_if()` function for this.

The `na_if()` function is a tidyverse function that converts the specified value to NA.

> As an example of `na_if()`, run the code block below:

```{r,eval=FALSE}

penguins <- mutate(mypenguins, bill_length_mm = na_if(bill_length_mm, -999))

```

In other circumstances, one might want to replace a missing data point with an expected value of that data point (assuming the dispersion of that variable is not too large). For example, if we ignore for the moment that morphological measurements are only missing for Adelie penguins for which the value for *sex* is also missing, we can take the `adelie` tibble and use the `replace_na()` function to replace missing values for *body_mass_g* with its median value, which is basically the inverse of `na_if()`.

> To replace NAs for *body_mass_mm* with the median value in a new tibble, run the following code block:

```{r}
nolengthNAs <- mutate(adelie, body_mass_g = replace_na(body_mass_g, median(body_mass_g, na.rm = TRUE)))
head(nolengthNAs)
```

In this case, we have created a new tibble, as we probably don't want to overwrite the original data, in case we want to undo the conversion of missing data to the median value. Notice, as with earlier use of summary statistics functions, we include the `na.rm=TRUE` statement. Otherwise, we would be replacing NA with NA, as applying `median()` to a vector containing NAs returns NA as its value! Of course, the calculation we just performed doesn't make much actual biological sense, because we are taking the median of \*body_mass_g" across both sexes in the tibble. In other contexts where other group-labeling variables such as sex aren't missing, one can perform more complex operations, e.g. if sex wasn't missing we could replace missing values with sex-specific median values. We will get into such more complex operations a bit later in the workshop.

Let's try to create a new variable that represents a Z score, i.e. a transformation of a set of measurements that represents the number of standard deviations it is away from the mean (and in what direction). Above, we have seen the `mean()` and `sd()` functions. A Z-score for an observation is simply (observation - mean of observations) / standard deviation of observations, with the distribution of Z being standard normal, i.e. mean == 0 and standard deviation == 1. Using the *adelie* tibble, we can create a new variable *body_mass_Z* that represents a Z-transformation of body mass for male Adelie penguins. We will write the output to a new tibble *adelie_Z*. There are really three ways to generate the new Z-score variable. The first, is to write the calculation of the Z-score from inside of the `mutate()` function. To do this ...

> Run the code block below to create the Z-score variable:

```{r}

adelie_Z <- mutate(adelie, body_mass_Z = (body_mass_g - mean(body_mass_g, na.rm=TRUE)) / sd(body_mass_g, na.rm=TRUE))

```

So, we are calculating the mean and sd of *body_mass_g* once, and using those values to apply the Z-score equation to each row of the *body_mass_g* vector and storing the output in the new variable *body_mass_Z*. Remember, unless you are certain there are no missing values in the variable vectors being used for calculations, when the option is available, it is best to invoke `na.rm=TRUE`.

To convince yourself that Z is in fact a standard normal distribution (mean=0, sd=1), check the mean and standard deviation of this new variable. Also, remember, statistical operations need to filter out missing values otherwise they will return *NA*.

> To get those summary statistics, run the code block below. Note that due to floating point errors, 0 may instead show up as a very small number.

```{r}

mean(adelie_Z$body_mass_Z, na.rm=TRUE)
sd(adelie_Z$body_mass_Z, na.rm=TRUE)

```



> **New variables creation exercise 2:** Can you think of another way to generate the Z-score column? Hint: you can create new variables for the constituent parts of the Z-score function, and then perform the calculation using those constituent parts. Try doing that, updating *adelie_Z* with the new variables. It should take three separate mutate commands. Write the Z-score to the new variable *z2*

```{r}


```

To confirm this produces the same output as the first implementation, calculate the mean and standard deviation of *z2* (these should also be 0 and 1, respectively).

```{r}


```

The distinction between these two approaches is that, in the first case, one is recycling the mean and standard deviation values (effectively vectors of size 1 each) in applying them to every element of *body_mass_g* in calculating the Z-score values in the respective rows. In the second case, because we have constructed mean and standard deviation variables of equal length to *body_mass_g*, we are doing mathematical operations on same-sized vectors. Of course, one might not want to store the same (e.g.mean) value over thousands of rows. The point here is to reinforce the idea that the same result can be achieved by constructing vectors of different lengths.

### Tibbles won't recycle vectors of length \> 1

As a side note to this, it is important to know that mutate requires that the object passed to the new column name argument is a vector of length 1 or a vector of length equal to the number of rows in the tibble. If you try to recycle a vector of length 2 in a mutate command, you'll get an error:

```{r, eval=F}

mutate(adelie, error = c(1,2))

```

## Using variable names to merge tables

Often at the beginning of a project, you have data from multiple sources that you want to merge, but are stored in different tables. For example, you might have visual measurements like color for a group of animals in one table and quantitative measurements like weight for the same animals in another table. In the best case, every animal is represented in both tables, i.e. each row in one table is associated with a row in the other table. Of course, it is also possible that one table has missing data for some observations - maybe the weight measurements wasn't taken for some animals - and that breaks the one-to-one relationship. In either case, we can merge tables by performing "mutating joins". In tidyverse, joins are performed one pair of tables at a time, such that, if you have tables x,y, and z, you must first join x and y to produce a new table xy, then join xy and z to create xyz. As we shall see below, a requirement of such joins is that any two tables to be joined contain unique observations on the same variable.

### Left joins

Left joins are a common operation, where given tables x and y, you only want to retain all rows in x, regardless if there is matching data in y: missing data in y will get returned as NAs in the new table. This sort of join is called left join, because the 1st table (the one on your left) is having data added to it from the table on the right. 

```{r}

penguins <- mutate(penguins, obs_number=row.names(penguins))

```

Now, we'll create some sub-tables with *select*

```{r}

x <- select(penguins, obs_number,body_mass_g)
y <- select(penguins, obs_number, species, sex)
xy <- left_join(x,y)
glimpse(xy)

```

Tidyverse determines on the fly that the two tables share a column name, and assumes that they contain the same data. If your collaborator used a different column name for *observation_number*, one will need to tell the join function which columns match. So, if we change the column name for the observation in table x, we can still perform the join as follows:

```{r}

x <- rename(x,obs = obs_number)
xy <- left_join(x, y, by = join_by(obs == obs_number))
glimpse(xy)

```

Other useful flavors of join are *inner_join*, which only keep observations observed in both tables, and *full_join*, which keeps all observations in both tables: data in x, but missing in y; data in y, but missing in x, data in both x and y.

## Converting wide to long format

While converting from wide to long is often a step for tidying untidy data. While the `penguins` data.frame object was already tidy, it can often be the case that a data table was created with different column variables that represent different types of the same class of measurement. For example, repeated weight measurements of a laboratory organism undergoing an experimental treatment might be recorded with a separate column for each measurement date. From a statistical and plotting perspective, date is a factor, and it can be more convenient to have all the weights in a single column, and a separate column for measurement date. In addition, it can make it easier to inspect data if there are fewer columns to look at ... otherwise, you are panning back and forth.

As we shall see in Part 3 of this workshop, having the actual measurements aggregated into a single column can make for easier plotting, if we also have an additional column that consists of a factor that describes which measurement it is. So, we are going to combine the three morphological measurements on bill length, bill depth, and flipper length into one column, making the table less wide and more long. Admittedly, with three columns converted to two, it isn't a big gain in ease of viewing, but it demonstrates the principles that can be applied when far more columns need to be merged.

```{r}

# First, we need to make each row have a unique ID. This is another way to do it
penguins <- mutate(penguins, id = row_number())
# Then, we use the function pivot_longer() to merge columns together
penguins_long <- pivot_longer(penguins,c(bill_length_mm,bill_depth_mm,flipper_length_mm),names_to="morphtype",values_to="mm")

```

In this operation, we specify the data frame being transformed, as well as a vector of column names for the columns we wish to merge. *names_to* is the new column that we are creating that will contain the old column name (of those column names merged), while *values_to* is the name of the new column that will store the actual measurement data.

Again, let's take a look and see that it worked!

```{r}

glimpse(penguins_long)

```

One thing we can see is that the new values in *morphtype* are unnecessarily long, because they all contain "\_mm" embedded in the label ... but we know the data are measured in millimeters, hence the name of the new column *mm*. So, we can use `mutate()` and `str_replace()` to strip that suffix off of the values:

```{r}

penguins_long <- mutate(penguins_long, morphtype = str_replace(morphtype, "_mm", ""))

```

In other scenarios, converting to long format is important for tidying data. A good example is the `USPersonalExpenditure` base R dataset.
```{r}
head(USPersonalExpenditure)
```

This dataset is stored as a matrix object, in which the type of household expense is a row name, and there are columns for each year. Ideally, all the expense amounts would be in the same column and year would be a separate variable. To clean up this data, we can do this:

```{r}
myUSPersonalExpenditure <- as.data.frame(USPersonalExpenditure)
myUSPersonalExpenditure$expenditure <- row.names(myUSPersonalExpenditure)
USPersonalExpenditure_long  <- as_tibble(myUSPersonalExpenditure) %>%
                               pivot_longer(cols=starts_with("19"),names_to="year",values_to="amount")

USPersonalExpenditure_long
```


It should be noted, that there are other, more complicated ways of performing wide-to-long operations. For more information, see the tidyverse documentation for [pivot_longer :octicons-link-external-24:](https://tidyr.tidyverse.org/reference/pivot_longer.html){:target="\_blank"}, or use R help to get more information displayed inside Rstudio.

## Multi-step data processing with pipes

Thus far, we have demonstrated how to execute particular *tidyverse* functions as distinct data-processing steps. However, most of the time one wants to perform a sequence of data-processing operations that require more than one R function. Without a means to chain these steps together, one would have to deploy a particular R function on a tibble, redirect the output to a new (or the same input) tibble, then deploy the next R function on the new tibble, etc. Or, one could try to nest sequential function executions within other function executions, e.g. `new_tibble <-(function2(function1(original_tibble)))` which can get unwieldy and error-prone very quickly. The good news is, *tidyverse* had a way of chaining together data processing steps that is analogous to pipes in the bash shell ... and guess what? They are also called pipes. The general syntax of using pipes is:

```         
tidy_data <- function1(data, args) %>% function2(args)
```

The `%>%` is the pipe, and it redirects the output of function1 to function2.

The pipe implicitly fills the first unnamed argument of the right hand function with the output of the left hand function. So in this case, the output of function1 is implicitly assigned to the first argument of function2. Because tidyverse functions always take the input data as their first argument, this is very convenient.

In the above command structure the final output of data processing with `function1` and `function2` gets directed to *tidy_data*, a new cleaned tibble.

Now, as a more concrete example, let's see how we can use `select()` and `filter()` together with a pipe.

> To combine `select()` and `filter()` operations to creata new tibble called *new_data*, run the code block below:

```{r}

new_data <- select(penguins, year, species, sex, body_mass_g) %>% filter(species=="Adelie", year == 2008)

```

In this example, we pipe together the two function calls we executed earlier, but all in one command line.

> **pipe exercise:**
>
> 1.  Try creating a more complex piped workflow, calling in sequence the `filter()` and `select()` commands above, followed by using `mutate()` to extract Adelie penguins and then calculate a body size (i.e. mass) normalized flipper_length.

```{r}


```

> 2.  Try experimenting with different row and column selections, and new variable creations with these three functions. You can even switch the order, e.g. create a new synthetic variable then applying `filter()` to it.

As a side note, you sometimes want to pipe the output of one function to the input of another function when the second function DOESN'T use the first argument for data. A common example of this is the function `lm()`, which is used to fit a linear model to data. The first argument to lm is the formula you want to fit, and the second argument is the data. There are two common options to still use a pipe here:

Because the output of the left hand function is piped to the first *unnamed* argument, if you name every argument before data, you'll be okay: `penguins %>% filter_command() %>% lm(formula = x ~ y)`

You can reference the piped output via the special variable `.` in the right hand function: \`penguins %\>% filter_command() %\>% lm(x \~ y, data = .)

## Finding unique values

There are *tidyverse* flavors of standard R functions. For example, to get the unique values for a column, instead of using `unique()`, we can use the `dplyr` function `distinct()`.

> To get the unique values for year, run the command below:

```{r}

distinct(penguins, year)

```

This function requires that you supply a tibble and column name for which you want to get the unique values. It does not behave as you would like if you simply supply the vector `penguins$year`.

## *Tidyverse* functions for getting data summaries

Yesterday, we showed you how the `summary` function will print out basic summary statistics for all columns in a data frame. Today, we introduce the `dyplr` function `summarize()` for generating specific statistics on one or more variables. For example, to get the number of individuals for which unique, individual ids are available (derived from pit tagging that started only in 2007).

> To generate this data summary, run the command below:

```{r}

summarize(penguins, n=n_distinct(species))

```

This function call returns a 1x1 tibble *n*.

Ignoring the fact for the moment that the data set is comprised of three different species, one could also obtain a summary statistic on variables, for example, mean size and weight.

> To generate a summary of mean bill length and body mass for Adelie penguins, run the command below:

```{r}

summarize(adelie, meanbill=mean(bill_length_mm, na.rm=TRUE), meanmass=mean(body_mass_g, na.rm=TRUE))

```

Perhaps we are getting tired of always typing `na.rm = TRUE`, and instead want to first filter our dataset to remove rows that contain missing values. Tidyverse contains a nice function, called `drop_na()`, that does just this. By default it looks for missing values in *every* column in in our tibble, but if you look at `?drop_na` you'll see it has one of those `...` arguments, so we can pass it column names to check.

> **As an exercise**, can you rewrite the summarize function above, but remove missing values before piping to summarize. Leave off the `na.rm=TRUE` to make sure it worked!

```{r}


```

You may have noticed that these two ways of handling NAs lead to different results from summarize. This is due to the fact that the first approach will compute statistics from all values that aren't NAs, regardless if other variables for that observation have missing data. In the second, we drop all observations that have missing data for any variable. Depending upon the particular data set and planned downstream analyses, there may be good reasons for choosing one approach over the other.

### Using `group_by()` with `summarize()`

In most data sets, summarizing across all observations (rows) may not be particularly informative, because what one usually wants to do is to get information specific to different groups of observations, e.g. those in different habitats, exposed to different experimental treatments, etc. Thus, in most circumstances one will want to define those groups for a tibble. One can do this using the `group_by()` function, which takes as it's first argument the name of the data frame, and the following arguments are variables to group by; these can then be followed by other keyword arguments). An obvious grouping for the *penguins* tibble is species.

> To obtain this grouping, run the command block below:

```{r}

by_species <- group_by(penguins, species)
by_species

```

Notice that in the header of the *by_species* tibble there is now an indication that there are four groups. It is important to note that `group_by()` does not change the data in the tibble: there is no column that specifies the groups. But wait, we know there are only three species in the data set. Let's check to see what the values are for species.

A feature of the penguins data is that there are a number of entries for which the sex and other measurements were not taken, i.e. are missing. While in this data set, if sex is missing the other measurements tend to be missing as well, there are cases where individual measurements might be missing and using the omnibus `drop_na` function might discard useful data. In which case one can filter on a particular missing column.

So, if we look at the sex variable:

```{r}
distinct(adelie,sex)
```

we see that *NA* is one of the values. We can fix this, so that, were we to group by sex, we won't create a group for *NA*.

> To generate grouped data for Adelie penguins by sex after removing rows with no sex information, run the following code block:

```{r}

adelie_by_sex <- filter(adelie, is.na(sex)==FALSE) %>% group_by(sex)
adelie_by_sex
```
... and, as a sanity check:

```{r}
distinct(adelie_by_sex,sex)
```


Now, let's calculate the mean by species for *body_mass_g* and *bill_length_mm*. If you're unsure whether there are NA values in these two columns, you can use the `drop_na()` function to remove rows which contain ANY NA values. To see how you might do this in one command using pipes, we'll start from *penguins*.

> Run the following code block:

```{r}

# starting with the penguins table
penguins %>% 
  # select only the columns we want
  select(species, body_mass_g, bill_length_mm) %>%
  # drop the rows with NA in ANY selected column
  drop_na() %>%
  # group table by species
  group_by(species) %>%
  # summarize the mean body mass, bill length, and number of observations in each group
  summarize(mean_weight=mean(body_mass_g), mean_bill_length=mean(bill_length_mm), num=n())

```

We have now produced mean statistic by species.

> `group_by()` **exercise:**
>
> Try building a tibble grouped by *species* and *sex*, then use summarize to get the mean and standard deviation of body mass. But first, get rid of missing values for both of these variables (hint: use `drop_na()`) and **DON'T** make changes to the *penguins* tibble.

```{r}

# start with the penguins table

  # we only want the columns species, section, and weight_g
 
  # remove NAs

  # group by species and section
 
  # summarize mean weight, standard deviation of weight, and number of observations in each group
  

```

You will notice that this has produced a warning message : `summarize()` has grouped output by 'species'. You can override using the `.groups` argument. The output of summarize is itself a tibble, which is grouped. However, we can't group it by both species and sex, since these are now unique combinations with only one row per pair. So summarize() is telling us it has chosen to group the output tibble by the `species` variable not the `section` variable.

## End of Part 2

------------------------------------------------------------------------
